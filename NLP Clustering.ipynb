{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fae88e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import re\n",
    "import pygsheets\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "import paddle\n",
    "paddle.enable_static\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "376f8d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sheet(sheet_url):\n",
    "    # Authenticate with the service account credentials\n",
    "    try:\n",
    "        gc = pygsheets.authorize(client_secret='client_secret.json')\n",
    "    except:\n",
    "        print('get data from local')\n",
    "        return list(pd.read_csv('nlp.csv', header=None)[0])\n",
    "\n",
    "    # Open the public Google Sheet using its URL\n",
    "    sheet = gc.open_by_url(sheet_url)\n",
    "\n",
    "    # Select the first sheet (index 0)\n",
    "    worksheet = sheet[0]\n",
    "\n",
    "    # Get all values from the worksheet as a 2D list\n",
    "    values = worksheet.get_col(1)\n",
    "    non_empty_values = [value for value in values if value]\n",
    "\n",
    "    return non_empty_values\n",
    "\n",
    "data = read_sheet('https://docs.google.com/spreadsheets/d/1nqvhZIGhOaH93mAqMzcpuRwl-p1lbDw34hWbxuEYRho')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5039991d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['玻尿酸是什麼', '玻尿酸是什麼做的？', '玻尿酸 對身體好嗎？']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d2e1bb",
   "metadata": {},
   "source": [
    "# Jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdae952a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Import error, cannot find paddle.fluid and jieba.lac_small.predict module. Now, back to jieba basic cut......\n",
      "[2023-07-18 01:24:27,919] [   DEBUG] _compat.py:50 - Import error, cannot find paddle.fluid and jieba.lac_small.predict module. Now, back to jieba basic cut......\n",
      "Building prefix dict from /Users/lytt/Desktop/zeroone/NLP Clustering/dict.txt.big ...\n",
      "[2023-07-18 01:24:27,921] [   DEBUG] __init__.py:113 - Building prefix dict from /Users/lytt/Desktop/zeroone/NLP Clustering/dict.txt.big ...\n",
      "Loading model from cache /var/folders/kd/bdrqt3tj351gf36v20dm1wqr0000gn/T/jieba.u88881455df339fc89de811b30045d662.cache\n",
      "[2023-07-18 01:24:27,922] [   DEBUG] __init__.py:132 - Loading model from cache /var/folders/kd/bdrqt3tj351gf36v20dm1wqr0000gn/T/jieba.u88881455df339fc89de811b30045d662.cache\n",
      "Loading model cost 0.490 seconds.\n",
      "[2023-07-18 01:24:28,411] [   DEBUG] __init__.py:164 - Loading model cost 0.490 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "[2023-07-18 01:24:28,412] [   DEBUG] __init__.py:166 - Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "jieba.enable_paddle()\n",
    "jieba.set_dictionary('dict.txt.big')\n",
    "jieba.add_word('玻尿酸')\n",
    "textlist = [jieba.lcut(text, use_paddle=True) for text in data]\n",
    "newtextlist = []\n",
    "newtextlist = [[word for word in text if word != '' and word != ' '] for text in textlist]\n",
    "newtextlist = [' '.join(i) for i in newtextlist]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70e4ad82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['玻尿酸 是 什麼', '玻尿酸 是 什麼 做 的 ？', '玻尿酸 對 身體 好 嗎 ？']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newtextlist[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec02523",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a1b492",
   "metadata": {},
   "source": [
    "## Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90b52440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.corpora import WikiCorpus\n",
    "# # Download from https://dumps.wikimedia.org/zhwiki/20230701/\n",
    "# wiki_corpus = WikiCorpus('zhwiki-20230701-pages-articles.xml.bz2', dictionary={})\n",
    "# text_num = 0\n",
    "\n",
    "# with open('wiki_text.txt', 'w', encoding='utf-8') as f:\n",
    "#     for text in wiki_corpus.get_texts():\n",
    "#         f.write(' '.join(text)+'\\n')\n",
    "#         text_num += 1\n",
    "#         if text_num % 10000 == 0:\n",
    "#             print('{} articles processed.'.format(text_num))\n",
    "\n",
    "#     print('{} articles processed.'.format(text_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3692029e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from opencc import OpenCC\n",
    "\n",
    "# # Initial\n",
    "# cc = OpenCC('s2t')\n",
    "\n",
    "# # Tokenize\n",
    "# with open('wiki_text_seg.txt', 'w', encoding='utf-8') as new_f:\n",
    "#     with open('wiki_text.txt', 'r', encoding='utf-8') as f:\n",
    "#         jieba.enable_paddle()\n",
    "#         jieba.set_dictionary('dict.txt.big')\n",
    "#         jieba.add_word('玻尿酸')\n",
    "#         text_num=0\n",
    "#         for times, data in enumerate(f, 1):\n",
    "#             text_num += 1\n",
    "#             if text_num % 10000 == 0:\n",
    "#                 print('{} articles processed.'.format(text_num))\n",
    "#             data = cc.convert(data)\n",
    "#             data = jieba.lcut(data, use_paddle=True, cut_all=False)\n",
    "#             data = [word for word in data if word != ' ']\n",
    "#             data = ' '.join(data)\n",
    "\n",
    "#             new_f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2096d5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Settings\n",
    "# seed = 666\n",
    "# sg = 0\n",
    "# window_size = 10\n",
    "# vector_size = 100\n",
    "# min_count = 1\n",
    "# workers = 8\n",
    "# epochs = 5\n",
    "# batch_words = 10000\n",
    "\n",
    "# train_data = word2vec.LineSentence('wiki_text_seg.txt')\n",
    "# model = word2vec.Word2Vec(\n",
    "#     train_data,\n",
    "#     min_count=min_count,\n",
    "#     vector_size=vector_size,\n",
    "#     workers=workers,\n",
    "#     epochs=epochs,\n",
    "#     window=window_size,\n",
    "#     sg=sg,\n",
    "#     seed=seed,\n",
    "#     batch_words=batch_words\n",
    "# )\n",
    "\n",
    "# model.save('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "755e18f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model = word2vec.Word2Vec.load('word2vec.model')\n",
    "except FileNotFoundError:\n",
    "    print('please uncomment the code above and train again')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a9ef904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "副作用 好處 : 0.48933536\n",
      "副作用 功效 : 0.653795\n",
      "副作用 後遺症 : 0.68280816\n",
      "好處 功效 : 0.525196\n",
      "好處 後遺症 : 0.27764294\n",
      "功效 後遺症 : 0.44948134\n"
     ]
    }
   ],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from itertools import combinations\n",
    "\n",
    "def cossim(a, b):\n",
    "    cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "    return cos_sim\n",
    "\n",
    "for a, b in combinations(['副作用', '好處', '功效', '後遺症'],2):\n",
    "    v_a = model.wv[a]\n",
    "    v_b = model.wv[b]\n",
    "    print(a, b, ':', cossim(v_a, v_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7757385",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_vectors = []\n",
    "for text in newtextlist:\n",
    "    wordlist = text.split(' ')\n",
    "    wordlist.remove('玻尿酸')\n",
    "    sent_vector = np.zeros((model.vector_size,), dtype=np.float32)\n",
    "    for word in wordlist:\n",
    "        try:\n",
    "            sent_vector+=model.wv[word]\n",
    "        except:\n",
    "            pass\n",
    "    sent_vectors.append(list(sent_vector/len(wordlist)))\n",
    "sent_vectors = np.array(sent_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b2a5bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153, 100)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b8b0d4",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd255bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import operator\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy\n",
    "\n",
    "def IDF(corpus, unique_words):\n",
    "    idf_vals = {}    \n",
    "    total_docs = len(corpus) \n",
    "    for word in unique_words: \n",
    "        cnt = 0\n",
    "        for row in corpus:\n",
    "            if word in row.split(\" \"): \n",
    "                cnt+=1 \n",
    "        idf_vals[word] = 1 + math.log((1+total_docs)/(1+cnt)) \n",
    "    return idf_vals\n",
    "\n",
    "def fit(dataset):\n",
    "    unique_words = set() \n",
    "    for row in dataset:\n",
    "        for word in row.split(' '):\n",
    "            unique_words.add(word)\n",
    "    unique_words = sorted(list(unique_words))\n",
    "    vocab = {j:i for i,j in enumerate(unique_words)}\n",
    "    idfs_ = IDF(dataset, unique_words)\n",
    "    return vocab, idfs_\n",
    "\n",
    "def transform(dataset, features, idfs_):\n",
    "    sparse_matrix = csr_matrix((len(dataset), len(features)), dtype=float)\n",
    "    for row in range(0, len(dataset)):\n",
    "        word_count = Counter(dataset[row].split(' '))\n",
    "        for word in dataset[row].split(' '):\n",
    "            if word in list(features.keys()):\n",
    "                tf = word_count[word] / len(dataset[row].split(' '))\n",
    "                tfidf = tf * idfs_[word]\n",
    "                sparse_matrix[row, features[word]] = tfidf\n",
    "    output = normalize(sparse_matrix, norm='l2', axis = 1, copy=True, return_norm=False)\n",
    "    return output\n",
    "\n",
    "def get_tfidf(textlist):\n",
    "    features, idfs_ = fit(textlist)\n",
    "    tfidf_matrix = transform(textlist, features, idfs_)\n",
    "    feature_dict_custom = []\n",
    "    tfidf = tfidf_matrix.toarray()\n",
    "    for i in range(len(tfidf)):\n",
    "        feature_dict_custom.append({key: value for key,value in zip(features.keys(), tfidf_matrix)})  \n",
    "    return tfidf, feature_dict_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39c2ccf2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_jieba, feature_jieba = get_tfidf(newtextlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7775dbe3",
   "metadata": {},
   "source": [
    "# Concatenate TFIDF and Average Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09e673f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153, 154)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_feat = np.concatenate([sent_vectors, tfidf_jieba], axis=1)\n",
    "all_feat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1335ec52",
   "metadata": {},
   "source": [
    "\n",
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd85fcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_result(labels):\n",
    "    clusters = [[] for _ in range(NUM_CLUSTERS)]\n",
    "\n",
    "    for idx, c in enumerate(labels):\n",
    "        clusters[int(c)].append(data[idx])\n",
    "\n",
    "    for c, result in enumerate(clusters):\n",
    "        print('Cluster {}: {}'.format(c, ' '.join(result)))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd73a9f",
   "metadata": {},
   "source": [
    "## KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d86b3a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: 玻尿酸可以用喝的嗎？ 吃玻尿酸有什麼好處？ 玻尿酸用喝的有效嗎？ 玻尿酸用喝的有效嗎？ 玻尿酸可以用喝的嗎？ 玻尿酸吃的有效嗎？ 什麼食物有玻尿酸？ 什麼食物有玻尿酸？ 玻尿酸可以用喝的嗎？ 什麼食物有玻尿酸？ 什麼食物有玻尿酸？ 吃玻尿酸有什麼好處？ 玻尿酸用喝的有效嗎？ 玻尿酸可以喝嗎？ 玻尿酸可以用吃的嗎？\n",
      "\n",
      "Cluster 1: 玻尿酸一天吃多少？ 玻尿酸一天吃多少？ 口服玻尿酸怎麼吃？ 玻尿酸一天吃多少？ 玻尿酸一天吃多少？ 玻尿酸一天吃多少？ 玻尿酸一天吃多少？ 玻尿酸一天吃多少？ 玻尿酸一天吃多少？ 玻尿酸一天吃多少？ 玻尿酸一天吃多少？ 玻尿酸一天吃多少？ 玻尿酸一天吃多少？ 口服玻尿酸怎麼吃？\n",
      "\n",
      "Cluster 2: 玻尿酸 對身體好嗎？ 打玻尿酸會傷身體嗎？ 打玻尿酸會傷身體嗎？ 打玻尿酸會傷身體嗎？ 癌症可以打玻尿酸嗎？ 打玻尿酸多久會自然？ 玻尿酸多久可以打一次？ 打玻尿酸會傷身體嗎？ 玻尿酸怎麼補充？ 玻尿酸打完多久自然？ 打玻尿酸會傷身體嗎？ 膝蓋打玻尿酸後會痛嗎？ 膝蓋打完玻尿酸可以運動嗎？ 玻尿酸怎麼補充？ 打玻尿酸會傷身體嗎？ 玻尿酸打完多久自然？ 打玻尿酸會傷身體嗎？ 膝蓋打玻尿酸後會痛嗎？ 膝蓋打完玻尿酸可以運動嗎？ 玻尿酸怎麼補充？ 打玻尿酸會傷身體嗎？ 玻尿酸怎麼補充？ 癌症可以打玻尿酸嗎？ 打玻尿酸多久會自然？ 玻尿酸多久可以打一次？ 打玻尿酸會傷身體嗎？ 玻尿酸 對身體好嗎？ 癌症可以打玻尿酸嗎？ 打玻尿酸多久會自然？ 玻尿酸多久可以打一次？ 玻尿酸打完多久自然？ 打玻尿酸會傷身體嗎？ 打完玻尿酸幾天消腫？ 打完玻尿酸多久可以運動？\n",
      "\n",
      "Cluster 3: 玻尿酸有什麼好處？ 玻尿酸有什麼副作用？ 玻尿酸有什麼副作用？ 玻尿酸有什麼好處？ 玻尿酸會致癌嗎？ 玻尿酸有什麼副作用？ 玻尿酸會有後遺症嗎？ 玻尿酸有什麼副作用？ 玻尿酸會致癌嗎？ 玻尿酸會有後遺症嗎？ 玻尿酸有什麼副作用？ 玻尿酸會致癌嗎？ 玻尿酸有什麼好處？ 玻尿酸有什麼副作用？ 玻尿酸有什麼好處？ 玻尿酸有什麼副作用？ 玻尿酸有什麼後遺症？ 玻尿酸有什麼副作用？ 玻尿酸會有後遺症嗎？ 玻尿酸有什麼副作用？ 玻尿酸有什麼好處？ 口服玻尿酸有什麼功效？ 口服玻尿酸有什麼功效？ 口服玻尿酸有什麼功效？ 玻尿酸會有後遺症嗎？ 口服玻尿酸有什麼功效？ 玻尿酸會有後遺症嗎？ 玻尿酸有什麼副作用？ 玻尿酸有什麼好處？ 口服玻尿酸有什麼功效？ 玻尿酸會致癌嗎？ 玻尿酸有什麼後遺症？ 玻尿酸有副作用嗎？ 玻尿酸會有後遺症嗎？ 玻尿酸會致癌嗎？ 玻尿酸有什麼後遺症？ 玻尿酸會有後遺症嗎？\n",
      "\n",
      "Cluster 4: 玻尿酸是什麼 玻尿酸是什麼做的？ 玻尿酸是天然的嗎？ 玻尿酸是什麼成份做的？ 玻尿酸是天然的嗎？ 玻尿酸是尿嗎？ 玻尿酸是什麼成份做的？ 玻尿酸是天然的嗎？ 玻尿酸是什麼做的？ 玻尿酸是天然的嗎？ 玻尿酸是什麼成份做的？ 玻尿酸是天然的嗎？ 玻尿酸是什麼成份做的？ 玻尿酸是吃什麼的？ 玻尿酸是天然的嗎？ 玻尿酸是尿嗎？ 玻尿酸是什麼成份做的？ 玻尿酸是天然的嗎？ 玻尿酸是什麼做的？\n",
      "\n",
      "Cluster 5: 玻尿酸什麼時候喝最好？ 玻尿酸什麼時候喝最好？ 玻尿酸什麼時候喝最好？ 玻尿酸什麼時候喝最好？ 玻尿酸什麼時候喝最好？ 玻尿酸又叫什麼？ 玻尿酸什麼時候吃最好？ 玻尿酸什麼時候喝最好？ 玻尿酸什麼時間吃最好？ 玻尿酸什麼時間吃最好？ 玻尿酸何時喝？ 玻尿酸什麼時間吃最好？ 玻尿酸什麼時候喝最好？ 玻尿酸什麼時間吃最好？ 玻尿酸什麼時候喝最好？ 玻尿酸什麼時候喝最好？\n",
      "\n",
      "Cluster 6: 為何叫玻尿酸？ 膝蓋打玻尿酸有什麼後遺症？ 膝蓋打玻尿酸有什麼後遺症？ 為何叫玻尿酸？ 膝蓋打玻尿酸有什麼後遺症？ 玻尿酸硬塊多久消？ 膝蓋打玻尿酸有什麼後遺症？ 膝蓋打玻尿酸有後遺症嗎？ 打玻尿酸會有後遺症嗎？ 膝蓋打玻尿酸有什麼後遺症？ 玻尿酸硬塊多久消？ 膝蓋打玻尿酸有什麼後遺症？ 膝蓋打玻尿酸有後遺症嗎？ 打玻尿酸會有後遺症嗎？ 膝蓋打玻尿酸有什麼後遺症？ 膝蓋打玻尿酸有什麼後遺症？ 玻尿酸硬塊多久消？ 打玻尿酸會痛幾天？\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "NUM_CLUSTERS = 7\n",
    "\n",
    "kmeans = KMeans(n_clusters=NUM_CLUSTERS, random_state=0, n_init=\"auto\").fit(all_feat)\n",
    "labels_kmeans = kmeans.labels_\n",
    "cluster_result(labels_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22aca88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4ac3e17",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cecf76a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: 玻尿酸可以用喝的嗎？ 玻尿酸什麼時候喝最好？ 玻尿酸什麼時候喝最好？ 玻尿酸什麼時候喝最好？ 玻尿酸用喝的有效嗎？ 玻尿酸什麼時候喝最好？ 玻尿酸什麼時候喝最好？ 玻尿酸用喝的有效嗎？ 玻尿酸可以用喝的嗎？ 玻尿酸什麼時候吃最好？ 玻尿酸吃的有效嗎？ 玻尿酸什麼時候喝最好？ 玻尿酸什麼時間吃最好？ 玻尿酸什麼時間吃最好？ 玻尿酸可以用喝的嗎？ 玻尿酸什麼時間吃最好？ 玻尿酸什麼時候喝最好？ 玻尿酸什麼時間吃最好？ 玻尿酸什麼時候喝最好？ 玻尿酸什麼時候喝最好？ 玻尿酸用喝的有效嗎？ 玻尿酸可以喝嗎？ 玻尿酸可以用吃的嗎？\n",
      "\n",
      "Cluster 1: 玻尿酸 對身體好嗎？ 打玻尿酸會傷身體嗎？ 膝蓋打玻尿酸有什麼後遺症？ 打玻尿酸會傷身體嗎？ 膝蓋打玻尿酸有什麼後遺症？ 打玻尿酸會傷身體嗎？ 癌症可以打玻尿酸嗎？ 打玻尿酸多久會自然？ 玻尿酸多久可以打一次？ 膝蓋打玻尿酸有什麼後遺症？ 打玻尿酸會傷身體嗎？ 膝蓋打玻尿酸有什麼後遺症？ 打玻尿酸會傷身體嗎？ 膝蓋打玻尿酸有後遺症嗎？ 膝蓋打玻尿酸後會痛嗎？ 膝蓋打完玻尿酸可以運動嗎？ 膝蓋打玻尿酸有什麼後遺症？ 打玻尿酸會傷身體嗎？ 膝蓋打玻尿酸有什麼後遺症？ 打玻尿酸會傷身體嗎？ 膝蓋打玻尿酸有後遺症嗎？ 膝蓋打玻尿酸後會痛嗎？ 膝蓋打完玻尿酸可以運動嗎？ 膝蓋打玻尿酸有什麼後遺症？ 打玻尿酸會傷身體嗎？ 癌症可以打玻尿酸嗎？ 打玻尿酸多久會自然？ 玻尿酸多久可以打一次？ 膝蓋打玻尿酸有什麼後遺症？ 打玻尿酸會傷身體嗎？ 玻尿酸 對身體好嗎？ 癌症可以打玻尿酸嗎？ 打玻尿酸多久會自然？ 玻尿酸多久可以打一次？ 打玻尿酸會傷身體嗎？ 打完玻尿酸多久可以運動？\n",
      "\n",
      "Cluster 2: 為何叫玻尿酸？ 口服玻尿酸怎麼吃？ 為何叫玻尿酸？ 玻尿酸又叫什麼？ 玻尿酸怎麼補充？ 玻尿酸打完多久自然？ 玻尿酸硬塊多久消？ 玻尿酸怎麼補充？ 玻尿酸何時喝？ 玻尿酸打完多久自然？ 玻尿酸硬塊多久消？ 玻尿酸怎麼補充？ 玻尿酸怎麼補充？ 口服玻尿酸怎麼吃？ 玻尿酸打完多久自然？ 玻尿酸硬塊多久消？ 打完玻尿酸幾天消腫？ 打玻尿酸會痛幾天？\n",
      "\n",
      "Cluster 3: 玻尿酸是什麼 玻尿酸是什麼做的？ 玻尿酸是天然的嗎？ 玻尿酸是什麼成份做的？ 玻尿酸是天然的嗎？ 玻尿酸是尿嗎？ 玻尿酸是什麼成份做的？ 玻尿酸是天然的嗎？ 玻尿酸是什麼做的？ 玻尿酸是天然的嗎？ 玻尿酸是什麼成份做的？ 玻尿酸是天然的嗎？ 玻尿酸是什麼成份做的？ 玻尿酸是吃什麼的？ 玻尿酸是天然的嗎？ 玻尿酸是尿嗎？ 玻尿酸是什麼成份做的？ 玻尿酸是天然的嗎？ 玻尿酸是什麼做的？\n",
      "\n",
      "Cluster 4: 玻尿酸一天吃多少？ 玻尿酸一天吃多少？ 玻尿酸一天吃多少？ 玻尿酸一天吃多少？ 玻尿酸一天吃多少？ 玻尿酸一天吃多少？ 玻尿酸一天吃多少？ 玻尿酸一天吃多少？ 玻尿酸一天吃多少？ 玻尿酸一天吃多少？ 玻尿酸一天吃多少？ 玻尿酸一天吃多少？\n",
      "\n",
      "Cluster 5: 玻尿酸有什麼好處？ 玻尿酸有什麼副作用？ 玻尿酸有什麼副作用？ 玻尿酸有什麼好處？ 吃玻尿酸有什麼好處？ 玻尿酸有什麼副作用？ 玻尿酸有什麼副作用？ 玻尿酸有什麼副作用？ 玻尿酸有什麼好處？ 玻尿酸有什麼副作用？ 玻尿酸有什麼好處？ 玻尿酸有什麼副作用？ 玻尿酸有什麼後遺症？ 玻尿酸有什麼副作用？ 玻尿酸有什麼副作用？ 玻尿酸有什麼好處？ 什麼食物有玻尿酸？ 口服玻尿酸有什麼功效？ 什麼食物有玻尿酸？ 口服玻尿酸有什麼功效？ 口服玻尿酸有什麼功效？ 什麼食物有玻尿酸？ 口服玻尿酸有什麼功效？ 玻尿酸有什麼副作用？ 玻尿酸有什麼好處？ 什麼食物有玻尿酸？ 口服玻尿酸有什麼功效？ 吃玻尿酸有什麼好處？ 玻尿酸有什麼後遺症？ 玻尿酸有副作用嗎？ 玻尿酸有什麼後遺症？\n",
      "\n",
      "Cluster 6: 玻尿酸會致癌嗎？ 玻尿酸會有後遺症嗎？ 玻尿酸會致癌嗎？ 玻尿酸會有後遺症嗎？ 玻尿酸會致癌嗎？ 玻尿酸會有後遺症嗎？ 打玻尿酸會有後遺症嗎？ 玻尿酸會有後遺症嗎？ 打玻尿酸會有後遺症嗎？ 玻尿酸會有後遺症嗎？ 玻尿酸會致癌嗎？ 玻尿酸會有後遺症嗎？ 玻尿酸會致癌嗎？ 玻尿酸會有後遺症嗎？\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "NUM_CLUSTERS = 7\n",
    "hierarchical_cluster = AgglomerativeClustering(n_clusters=NUM_CLUSTERS, metric='euclidean', linkage='ward')\n",
    "labels_hc = hierarchical_cluster.fit_predict(all_feat)\n",
    "cluster_result(labels_hc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644a2ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c38a17d0",
   "metadata": {},
   "source": [
    "# Upload to google sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef2e3485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the sheet: NLP Clustering\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    gc = pygsheets.authorize(client_secret='client_secret.json')\n",
    "except:\n",
    "    print('no client_secret.json')\n",
    "\n",
    "# Open the Google Sheet by title\n",
    "sheet_title = 'NLP Clustering'\n",
    "# Find the Google Sheets file by name\n",
    "try:\n",
    "    sheet = gc.open(sheet_title)\n",
    "    print(\"Found the sheet:\", sheet.title)\n",
    "except pygsheets.SpreadsheetNotFound:\n",
    "    print(\"The sheet with the name '{}' was not found. Creating a new one.\".format(sheet_title))\n",
    "\n",
    "    # Create a new spreadsheet with the given name\n",
    "    sheet = gc.create(sheet_title)\n",
    "    print(\"New sheet created with the title:\", sheet.title)\n",
    "\n",
    "\n",
    "worksheet = sheet[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fd24fbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the range where you want to write the data, for example, starting from A1\n",
    "cell_range = 'A1:C' + str(len(data))\n",
    "\n",
    "column1 = data\n",
    "column2 = ['kmeans'] + [int(label) for label in labels_kmeans]\n",
    "column3 = ['hierarchical clustering'] + [int(label) for label in labels_hc]\n",
    "\n",
    "# Prepare the data as a list of lists to match the three columns\n",
    "data_to_write = [column1, column2, column3]\n",
    "data_to_write = [list(x) for x in zip(*data_to_write)]\n",
    "\n",
    "# Write the data to the worksheet\n",
    "worksheet.update_values(crange='A1', values=data_to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15735691",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zeroone",
   "language": "python",
   "name": "zeroone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
